<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mingyi Sun(孙茗逸)'s Homepage</title>

  <meta name="author" content="Mingyi Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/self.jpeg">
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Mingyi Sun(孙茗逸)</name>
                  </p>
                  <p style="text-align:center">
                    Email: sunmingyi[at]buaa.edu.cn / <a href="https://scholar.google.com/citations?user=_9QJ5xEAAAAJ&hl=en">Google Scholar</a> / <a href="https://smy17.github.io/Mingyi_Sun_CV.pdf">CV</a> / <a href="https://github.com/smy17">Github</a>
                  </p>
                  <p>
                    I am a final-year Master student at the Automation Science and
                      Electrical Engineering department of Beihang University (BUAA) </a>,
                    supervised by <a href="https://cqf.io/"> Prof. Qifeng Chen </a>. Prior to this, I got my Bachelor's
                    degree from Department of Computer Science at <a href="https://en.whu.edu.cn/"> Wuhan University </a> in 2018.
                  </p>  
                   
                  <p>
                    I'm interested in <strong>computational photography, image/video synthesis, 3D generative models and neural rendering.</strong>
                  </p>
                  <p>
                  <br>
                  <font color="#FF0000">
                  I'm expected to graduate in 2023 and looking for a job now. Please feel free to reach out if you have potential job opportunities.</font>
                  </p>



                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:60%;max-width:60%" alt="profile photo" src="./yue_wu.jpg" class="hoverZoomLink"></a>
                </td>

              </tr>
            </tbody>
          </table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Experience</heading>
                  <p>
                    Research Intern, SenseTime, Jul.2017 - Dec.2017. 
                    <br>
                    Working with <a
                    href="https://scholar.google.com/citations?user=KZn9NWEAAAAJ&hl=zh-CN"> Dr. Wentao Liu </a> and <a
                    href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en"> Dr. Chen Qian </a>.
                    <br>
                    <br> 
                    Research Intern, MSRA, Jan.2022 - now. 
                    <br> 
                    Working with <a href="https://jlyang.org/"> Dr. Jiaolong Yang
                    </a>, <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en"> Dr. Fangyun Wei </a>
                    and <a href="https://www.microsoft.com/en-us/research/people/xtong/"> Dr. Xin Tong </a> in vision computing
                    group.
                  
                  </p>
                </td>
              </tr>
            </tbody>
          </table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
               * indicates joint authors 
              </p>
            </td>
          </tr>
        </tbody>
        
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
          <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                    <source src='images/anifacegan.mp4'>
          </video>
                    </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars</papertitle>
              <br>
              <strong>Yue Wu</strong>, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng Chen, Xin Tong
              <br>
              <em>2022 Neural Information Processing Systems</em>, NeurIPS 2022<strong><font color="#FF0000">(Spotlight)</font></strong>
              <br>
              <a href="https://arxiv.org/abs/2210.06465">[PDF]</a>
              <a href="https://yuewuhkust.github.io/AniFaceGAN/">[Project]</a>
              <a href="images/AniFaceGAN.txt">[BibTeX]</a>
              <a onClick="alert('Contact wu.kathrina@gmail.com for the inference code and pretrained models!')">[Code]</a>
                <!-- <h4><strong>Code</strong></h4> -->
              <br>
              <p>We propose AniFaceGAN, an animatable 3D-aware GAN for multiview consistent face animation generation.</p>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/waterdrop.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes</papertitle>
              <br>
              Qiang Wen, <strong>Yue Wu</strong>, Qifeng Chen
              <br>
              <em>2023 IEEE International Conference on Robotics and Automation,</em> ICRA 2023
              <br>
              <!-- <a href="https://arxiv.org/abs/2206.07255">[PDF]</a> -->
              <!-- <a href="https://yuewuhkust.github.io/AniFaceGAN/">[Project]</a> -->
      <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <p>We propose a video waterdrop method achieving the best performance in complex real-world driving scenes .</p>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: 10%">
              <source src='./OVP_VFI/all.mp4'>
  </video>
        </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Optimizing Video Prediction via Video Frame Interpolation</papertitle>
              <br>
              <strong>Yue Wu</strong>, Qiang Wen, Qifeng Chen
              <br>
              <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022
              <br>
              <a href="https://arxiv.org/abs/2206.13454">[PDF]</a>
              <a href="https://yuewuhkust.github.io/OVP_VFI/">[Project]</a>
              <a href="https://github.com/YueWuHKUST/CVPR2022-Optimizing-Video-Prediction-via-Video-Frame-Interpolation/">[Code and Results]</a>
              <a href="images/OVP_VFI.txt">[BibTeX]</a>
              <br>
              <p>We propose an optimization framework for video prediction without external training.</p>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
              <img src='images/novel.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Embedding Novel Views in a Single JPEG Image</papertitle>
              <br>
              <strong>Yue Wu*</strong>, Guotao Meng*, Qifeng Chen
              <br>
              <em>2021 International Conference on Computer Vision</em>, ICCV 2021
              <br>
              <a href="https://cqf.io/papers/Embedding_Novel_Views_ICCV2021.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2108.13003">[arXiv]</a>
              <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a>
              <a href="images/embedding.txt">[BibTeX]</a>
              <br>
              <p>An interesting method to embedding novel views into a single image.</p>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/colorization.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Towards Photorealistic Colorization by Imagination</papertitle>
              <br>
              Chenyang Lei *, <strong>Yue Wu*</strong>, Qifeng Chen
              <br>
              <em>arxiv</em> 2021,
              <br>
              <!-- <a href="https://cqf.io/papers/Embedding_Novel_Views_ICCV2021.pdf">[PDF]</a> -->
              <a href="https://arxiv.org/abs/2108.09195">[arXiv]</a>
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <p>We propose a method to achieve outsanding colorization result by utilizing an imagination module.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/long_term.jpg' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Video Super-Resolution with Long-Term Self-Exemplars</papertitle>
              <br>
              Guotao Meng, <strong>Yue Wu</strong>, Qifeng Chen
              <br>
              <em>2023 IEEE International Conference on Robotics and Automation,</em> ICRA 2023
              <br>
              <!-- <a href="https://cqf.io/papers/Embedding_Novel_Views_ICCV2021.pdf">[PDF]</a> -->
              <a href="https://arxiv.org/abs/2106.12778">[arXiv]</a>
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <p>We propose Video SR method by utilizing self-exemplars in long-term frames.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/video_pred.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Future Video Synthesis with Object Motion Prediction</papertitle>
              <br>
              <strong>Yue Wu</strong>, Rongrong Gao, Jaesik Park, Qifeng Chen
              <br>
              <em>2020 Computer Vision and Pattern Recognition Conference</em>, CVPR 2020,
              <br>
              <a href="https://cqf.io/papers/Future_Video_Synthesis_With_Object_Motion_Prediction_CVPR2020.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2004.00542">[arXiv]</a>
              <a href="https://github.com/YueWuHKUST/FutureVideoSynthesis">[Code and Result]</a>
              <a href="./images/cvpr2020_videopred.txt">[BibTeX]</a>
              
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <p>An dedicated framework to decompose the scene and utilize spatial transformer to mimic the movement of rigid scenes.</p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/pose.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Towards Multi-Person Pose Tracking: Bottom-up and Top-down Methods</papertitle>
              <br>
              Sheng Jin,
                    Xujie Ma,
                    Zhipeng Han,
                    <strong>Yue Wu</strong>,
                    Wei Yang,
                    Wentao Liu,
                    Chen Qian,
                    Wanli Ouyang
              <br>
              <em>2017 International Conference on Computer Vision</em>, ICCV Workshops 2017,
              <br>
              <a href="https://jin-s13.github.io/papers/BUTD.pdf">[PDF]</a>
              <!-- <a href="https://arxiv.org/abs/2004.00542">[arXiv]</a>
              <a href="https://github.com/YueWuHKUST/FutureVideoSynthesis">[Code and Result]</a>
               -->
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <p>Ranked 2nd Places in ICCV Posetrack Challenge</p>
          </td>
      </tr>           
            
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/icme.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Saliency map generation based on saccade target theory</papertitle>
              <br>
              <strong>Yue Wu</strong>,
                    Zhenzhong Chen
              <br>
              <em>ICME</em> 2017,
              <br>
              <a href="https://ieeexplore.ieee.org/document/8019456">[PDF]</a>
              <!-- <a href="https://arxiv.org/abs/2004.00542">[arXiv]</a>
              <a href="https://github.com/YueWuHKUST/FutureVideoSynthesis">[Code and Result]</a>
               -->
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <!-- <p>Ranked 2nd Places in ICCV Posetrack Challenge</p> -->
          </td>
      </tr>                    
  


<!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Academic Services</heading>
        <p>
          Conference: ECCV, CVPR, ICRA 
          <br>
          <br>
        </p>
      </td>
    </tr>
  </tbody>
</table>           
   -->

           
            
                   
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Teaching</heading>
        <p>
          COMP5411: Computer Graphics, 2019
          <br>
          COMP2711H: Honors Discrete Mathematical Tools for Computer Science, 2020
          <br>
          COMP3511: Operating System, 2021
          <br>
          COMP5214: Advanced Deep Learning Architectures, 2022
        </p>
      </td>
    </tr>
  </tbody>
</table>



<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Honors and Awards</heading>
        <p>
          Postgraduate Scholarship, HKUST
          <br>
          National scholarship, Wuhan University
          <br>
          First-Class scholarship, Wuhan University
          <br>
          Best Head Movement Prediction Student Prize ICME Grand Challenge Salient360! 2017
          <br>
          Meritorious Winner Interdisciplinary Contest In Modeling (ICM) 2017
          <br>
        </p>
      </td>
    </tr>
  </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Academic Service</heading>
        <p>
          Reviewer for ECCV 2022, CVPR 2022, ICRA 2023, CVPR 2023, JMLR
          <br>
          <br>
        </p>
      </td>
    </tr>
  </tbody>
</table>

<a href="https://info.flagcounter.com/UQVZ"><img src="https://s11.flagcounter.com/count2/UQVZ/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>

</html>
